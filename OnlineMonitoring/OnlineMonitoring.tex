% Chapter on Online Monitoring, Data Quality Monitoring & Event Displays
% 10 pages

\graphicspath{{OnlineMonitoring/Figs/}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Online Monitoring and Event Displays for the 35~ton Experiment}\label{chap:OnlineMonitoring}

Monitoring of the data collected during the running of an experiment is imperative to ensure a high quality of data is maintained.  Such monitoring is often provided in real-time (`Online Monitoring'), summarising the data from the current run, or in near real-time (`Nearline Monitoring'), summarising data over runs from typically the previous day, week or month to represent the longer term fluctuations in the data quality.  The system developed to provide online feedback for the 35~ton Run II data taking period is discussed in this present section.

An event display, designed to illustrate physics events as they occur in the detector, is another desirable feature that is particularly useful during data collection.  A basic example of such a display is also produced by the monitoring system for the purposes of ensuring good quality physics data collection is maintained.

The system is designed to be flexible and provide prompt feedback for those operating the experiment.  It was thus included as part of the DAQ (Data Acquisition) system, \textit{lbne-artdaq}, discussed in Section \ref{sec:lbne-artdaq}.  The monitoring framework itself is the subject of Section \ref{sec:OnlineMonitoring}, with its two functions, data quality monitoring and producing online event displays, presented in Section \ref{sec:DQM} and Section \ref{sec:EventDisplay} respectively.  Finally, the web interface developed to allow synchronisation of this monitoring data to a dedicated web page for ease of access is briefly described in Section \ref{sec:WebInterface}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The DAQ Framework}\label{sec:lbne-artdaq}

Experiments at FNAL are migrating to \textit{artdaq}, a centrally-maintained data acquisition system built on the art framework utilised by all offline software written for experiments hosted at the lab.  The DUNE 35~ton experiment was one of the first to use this new software (only LArIAT had previously used it for data taking) and used an experiment specific system named lbne-artdaq.
%% \footnote{Since the formation of the DUNE experiment occurred only a few months before the running of the 35~ton, all online software maintained the use of the outdated `lbne' descriptor to prevent unnecessary potential problems associated with large scale code changes and alleviate the risk of further delays.  It should be again stressed that the 35~ton was recognised by the DUNE collaboration as an integral part of the DUNE plan and the use of \textit{lbne} was in no way an indication of a project associated only with the dissolved previous experiment!}.
A general overview of lbne-artdaq is shown in Figure \ref{fig:lbne-artdaq}.

\begin{figure}[ht]
\centering
  \includegraphics[width=16cm]{artdaqFramework.pdf}
  \caption[The \textit{lbne-artdaq} framework]{Overview of the \textit{lbne-artdaq} framework used for data acquisition by the DUNE 35~ton experiment.  See the text for a complete description.  [Thank John Freeman for this image... how to reference?]}
  \label{fig:lbne-artdaq}
\end{figure}

Data flows from left to right and pass through components common to most DAQ systems.  Closest to the detector components (i.e. the RCEs, SSPs and PTB [see Section \ref{sec:DetectorComponents}]) are the board readers which take the output from the firmware as soon as it is ready and sends it downstream to the event builders.  There exists a board reader for each of the detector components (totalling 24) and each is unaware of the existence of the others.  It is the job of the event builders to assemble a full `event' from these individual `fragments' passed on from each of the detector elements.  An event is complete once composed of a full set of fragments and the event builders will wait to receive them all before sending the data onwards to the aggregators.

There are two aggregators which take the full events but process them in very different ways.  All the data passes through only the first aggregator, whose function it is to write the output to disk and thus end processing by the DAQ.  The second aggregator receives no events but instead has access to the shared memory occupied by the data as it passes through the first aggregator; it is thus designed specifically for the purpose of monitoring and in no way affects the data or the output from the first aggregator.  It is within this second aggregator process that the online monitoring system described in the proceeding section is designed to run.

Each of the DAQ processes runs on a machine on the private DAQ network and is configured as normal within art (using the \textit{fhicl} (Fermilab Hierarchical Configuration Language) configuration language).  Two nodes on the main FNAL network (lbne-gateway01/02) provide access to these private machines, of which there are 7 (lbnedaq1-7), and contain all scripts and setup necessary to run through the DAQ via a command line interface.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Online Monitoring Framework}\label{sec:OnlineMonitoring}

The framework developed for the monitoring system had the following design goals:

\begin{itemize}
\item to be able to analyse the data read out of memory in its raw `DAQ format';
\item to be as computationally efficient as possible to allow for processing at the event rate (data taking rate);
\item to provide the flexibility for further monitoring plots to be added with ease;
\item to allow for use of an online event display to provide comprehensible images of the raw data.
\end{itemize}

In general, the final developed system succeeded in all these goals and provided invaluable information, becoming an integral tool in the commissioning and the data taking of the 35~ton.  An illustration of the framework is shown in figure \ref{fig:OnlineMonitoringFramework}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=12cm]{softwareFramework.eps}
  \caption[Software framework built for 35~ton Online Monitoring]{Demonstration of the framework designed for online monitoring in the DUNE 35~ton experiment.}
  \label{fig:OnlineMonitoringFramework}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monitoring Framework Design}\label{sec:MonitoringFrameworkDesign}

The setup consists of a central `module', \texttt{OnlineMonitoring\_module.cc}, which is configured within the art framework through its base class.  The OnlineMonitoring class controls the running of the system and owns instances of further classes, each designed for a specific purpose, controlling the data flow by calling the relevant methods when required.  Once an event has been obtained, the data for each component is processed and repackaged into RCEFormatter, SSPFormatter and PTBFormatter objects.  The purposes of this method are thus:
\begin{itemize}
\item to provide an interface between the raw data and the methods which analyse the data.  This is important as it provides a single point of maintenance for when formats change and allow for various `DAQ modes' to use the same analysis code;
\item to separate interaction with the DAQ from the handling of output data objects;
\item to facilitate random access of the data for more detailed analysis which would not be possible if just processing linearly.
\end{itemize}
The main drawback to performing this step is it requires all the data to be held in memory until the end of the event and represents basically the same information as initially present.  However, it was decided the advantages were worth the compromises in memory usage required and no problems were apparent during the course of the run except when operating at the very limits of the capability of the DAQ.

These reformatted data objects are then passed to the methods in the MonitoringData class for straight-forward analysis.  This class owns all of the data products which are output from the monitoring (e.g. histograms, graphs, trees and files) and deals with their filling and writing out when required.  This is discussed further in Section \ref{sec:DQM}.

The event display is handled by its own dedicated class, EventDisplay; this has methods for making the displays and saving them as an image in the correct place when required.  It is designed to accept the reformatted RCE object and presents the data in as meaningful way as possible; this is detailed fully in Section \ref{sec:EventDisplay}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Writing Monitoring Data}\label{sec:WritingMonitoringData}

The data objects are created new for each subrun and are written out at three points during data taking:
\begin{itemize}
\item an initial write out N seconds after the start of the subrun;
\item at frequent intervals during the subrun, every M seconds;
\item at the end of the subrun.
\end{itemize}
The parameters N and M are user defined and were set to 30 and 500 respectively for normal data taking.  The data products are only cleared at the end of a subrun, so any intermediate writing out of data simply refreshes the current plots.

The event displays are computationally expensive to make and so were only created once per subrun during normal running.  However, since a subrun was automatically stopped by the DAQ and a new one started once the output file had reached 5 GB in size, and (since zero suppression was not utilised at any point during the run) this occurred on average every four minutes, a new event display was made every few minutes.

All the output data are saved on a shared disk on the gateway DAQ machines for further use.  This is discussed in Section \ref{sec:WebInterface} below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \subsection{Configuring the Monitoring}\label{sec:MonitoringConfiguration}

%% [Possibly don't need this section...]
%% The system was designed to be flexible and many parameters were available to control the running of the monitoring.  These are listed and described below, with default parameter provided in brackets.

%% \begin{itemize}
%% \item TPCModuleLabel (``daq'') -- art module label for TPC data saved in the DAQ;
%% \item PhotonModuleLabel ([ ``sparseSsp'', ``daq'' ]) -- art module label for photon data saved in the DAQ;
%% \item TriggerModuleLabel (``daq'') -- art module label for counter data saved in the DAQ;
%% \item DetailedMonitoring (false) -- fills more, and usually more computationally expensive, data;
%% \item ScopeMonitoring (false) -- support for a different DAQ running mode, `scope mode';
%% \item DataDirPath (``/storage/data/'') -- path at which the data files are saved by the first aggregator;
%% \item MonitorSavePath (``/data2/lbnedaq/monitoring/'') -- location to save DQM data;
%% \item EVDSavePath (``/data2/lbnedaq/eventDisplay/'') -- location to save event displays;
%% \item PedestalFile (``/data2/lbnedaq/pedestal.csv'') -- location of the most recent pedestal file, containing pedestals for all channels.  This was used for pedestal subtraction when making event displays;
%% \item ImageType (``.png'') -- the format to save any images;
%% \item MonitoringRefreshRate (500) -- how often to write out the most recent monitoring data plots;
%% \item InitialMonitoringUpdate (30) -- how long after starting a new subrun to initially write out the data;
%% \item EventDisplayRefreshRate (60) -- how often to refresh the event display when not just making one per subrun;
%% \item LessFrequentFillRate (20) -- how often to fill the more computationally expensive plots;
%% \item DriftVelocity (0.9 \#mm/us) -- rough drift electron velocity, used for calculating the x coordinate for the event display;
%% \item CollectionPedestal (550) -- the default pedestal value if the file isn't readable;
%% \item MicroslicePreBuffer (5) -- how many microslices are saved by the RCEs before the one containing the trigger;
%% \item MicrosliceTriggerLength (5) -- the length of the trigger.
%% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Quality Monitoring}\label{sec:DQM}

The overarching aims of the online monitoring system was to provide direct feedback informing the operators of the quality of the data being taken.  This is vital for various different aspects of data taking, for example
\begin{itemize}
\item ensuring all detector components being used in the current run are receiving and processing data;
\item noting the TPC readout has entered the `high noise state` and acting accordingly; [MW: I will probably have explained what this is in a previous chapter!]
\item checking the trigger rates from the external cosmic muon counters are feasible.
\end{itemize}

The monitoring was diagonalised in a similar way to the DAQ readout with data from the TPC, photon detector and external counters processed separately.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{TPC Monitoring}\label{sec:TPCMonitoring}

Ensuring the high quality of TPC data involved mainly considering various distributions of the ADC values provided by the front-end boards, separated by channel, board and APA.  The mean and RMS of the ADC values for a given channel provides information such as the measured pedestal and the level of noise being read out.  The uncorrelated component of the noise can be monitored using the concept of `DNoise'; this considers the difference in ADC value between two neighbouring channels at a given readout time and represents the level of noise which would be impossible to remove by the use of coherent noise filters only.  Unfortunately, for the 35~ton, this uncorrelated component made up most of the noise across all channels (see Figure \ref{fig:DQMPlot1}).  FFTs of the signal waveforms, performed separately for each RCE, were also useful in monitoring bands of noise in frequency space.

Monitoring of various other problems, such as the `digitiser stuck code' issue, synchronisation concerns resulting in a different number of microslices being saved in corresponding RCE millislices, and the asymmetry of bipolar pulses, were added as these issues became apparent during the commissioning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Photon Detector Monitoring}\label{sec:PhotonMonitoring}

Analogously to the TPC situation, monitoring of the photon detectors mainly involved considering various ADC distributions separated by optical channel and by detector.  The peak height, pedestal and integral of each waveform were also considered as a function of channel to ensure each were operating consistently.

The triggers sent on by the SSPs were also studied; unfortunately due to the design of the monitoring framework (with it not guaranteed to receive each event), trigger rates were challenging to compute.  Eventually, it was decided to leave them in the monitoring but only consider the relative rates -- the monitoring code was used offline, processing closed files, to ensure all events were considered and determine accurate rates.  Along with the trigger rate, the number of triggers, the fraction of events containing a trigger and the number of readout ticks within each trigger were also considered.

During installation, one photon detector was erroneously left unconnected to its SSP and so was unavailable during the run.  This was discovered using the online monitoring -- unfortunately after the cryostat had been sealed however.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{External Counter Monitoring}\label{sec:CounterMonitoring}

Since monitoring the external counters primarily involves considering trigger rates, a similar problem to that encountered in the photon detector monitoring was faced.  A similar solution was agreed upon and the trigger rates were only considered relative to different counters.  For each counter, the hit rate and the average activation time were monitored to ensure counters in similar positions were recording similar cosmic muon data.  The number and type of payloads sent on from the PTB were also detailed so the amount of data, along with information about what the data are comprised of, can be monitored.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General Monitoring}\label{sec:GeneralMonitoring}

A variety of useful quantities not pertaining to any specific subcomponent were also monitored to assure smooth data taking.  These include the size of output files from recent runs, the average event size from recent runs, information about which detector subcomponents are taking data and the number of events seen by each and also synchronisation information between various detector components.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{DQM Plots}\label{sec:DQMPlots}

The DQM section of the online monitoring produced around 60 figures each time it is run, illustrating the data discussed in the previous sections.  It is unnecessary to reproduce many here [perhaps an appendix? I think unnecessary though] but a sample for reference are shown in Figure \ref{fig:DQMPlots}.

\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{DQM1.png}
    \caption{TPC noise. The total noise (RMS of the ADC values) is shown in blue and the uncorrelated component of this noise is shown in green.  This is determined by considering the `DNoise', the difference in ADC between neighbouring channels for any given tick.}
    \label{fig:DQMPlot1}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{DQM2.png}
    \caption{FFT of the waveform read out by the first RCE (channels 1--128).}
    \label{fig:DQMPlot2}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{DQM3.png}
    \caption{ADC values as a function of channel; incredibly useful plot containing the mean and RMS for all channels together.}
    \label{fig:DQMPlot3}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{DQM4.png}
    \caption{Subdetectors which are successfully collecting data.  In this particular run, it can be seen one quarter of the TPC readout was turned off, along with three photon detector readouts.}
    \label{fig:DQMPlot4}
  \end{subfigure}
  \caption[Selection of Data Quality Monitoring figures]{Demonstration of various plots used in the Data Quality Monitoring for the 35~ton. [Images are probably placeholders at the moment and should be reconsidered.]}
  \label{fig:DQMPlots}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Online Event Display}\label{sec:EventDisplay}

One of the highlights of being in ROC West (Remote Operation Control room at FNAL) during data taking was watching the online event display refresh with updated images representing cosmics passing through the detector.  The event display additionally allowed for straight-forward monitoring of the data -- high noise states, poor LAr purity and drift field problems were all immediately evident from the display.

Given the way in which data were read out of the detector, it proved challenging finding a way to represent this in a simple way that was comprehensible.  The construction of such a display is the subject of this section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Selecting the Data}\label{sec:SelectingEVDData}

[The data format will likely be described in the 35~ton section -- most of this will probably be moved up to that point and referenced at a later stage...]

The raw format for the TPC data is complicated and has many levels of structure.  Refer to Figure \ref{fig:DataFormat} during the following description.  The 2048 channels are readout out by 16 front-end boards (containing the cold electronics, including the digitisers), each processed by an RCE and then read into the DAQ by a board reader.  The format at this point is referred to as a millislice; there is a millislice for each of the detector components (RCEs, SSPs and PTB) and an `event' is a collection of all such millislices.  For the TPC data, a millislice contains all the information for 128 channels.  This data also has further substructure; a millislice is composed of N microslices, with each microslice containing M nanoslices.  A nanoslice contains 128 ADC values, representing one `tick' ($500$ ns) worth of data for 1/16th of the detector.  A microslice thus contains this information for a `drift window' (M ticks) and a millislice a collection (M) of drift windows.  For the normal data running, N was set to 20 and M 1000.

\begin{figure}[p]
  \centering
  \includegraphics[width=14cm]{data_format.eps}
  \caption[35~ton data format]{Demonstration of the format used in 35~ton raw data.  A `DAQ event' is composed of a single millislice from each component, each containing further substructure unique to the readout elements.}
  \label{fig:DataFormat}
\end{figure}

As the detector collects data, the RCEs continually create and save microslices to send to the DAQ to form a millislice.  These microslices are empty (contain no nanoslices) until a trigger is received, at which point nanoslices are made and saved within each microslice.  There is also a buffer in place to save a certain number of full microslices (microslices containing nanoslices) before the microslice containing the trigger.  A certain number of full microslices proceeding the trigger are also recorded by the RCEs.  During normal running, a `$4+1+10$' format was employed; four microslices containing nanoslices before the trigger was received, the microslice containing the trigger, and the ten following microslices.  It should be further noted that, since the DAQ was designed for continuous data readout, these microslices need not necessarily be within the same millislice: it is possible for the trigger to occur in microslice 18 of a certain millislice, resulting in the 15 filled microslices straddling successive millislices.  This is demonstrated in Figure \ref{fig:TriggeredEvent}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=16cm]{triggered_event.eps}
  \caption[Demonstration of how TPC data from a triggered event in a LArTPC is saved when employing a DAQ with continuous readout]{Demonstration of how TPC data is saved when using a DAQ designed for continuous readout.  The black arrows represent hypothetical triggers occurring within the duration of a particular millislice.  In each case, the 4 preceding microslices and the 10 proceeding microslices are filled with nanoslices and saved; all other microslices are saved with no nanoslices since they contain no useful data.  An example of such an event is shown occurring in millislice 0 in the figure.  As described in the text, a trigger can cause the useful microslices to straddle consecutive millislices; this is represented in the following millislices in the figure.}
  \label{fig:TriggeredEvent}
\end{figure}
Note this also results in real `physics events' being saved in separate `DAQ events'; for this reason a splitter/stitcher module has been designed to extract the actual triggered events from the raw data and repackage them into a useful event structure -- this is the first stage before all offline analysis with the 35~ton data.

Since the event display runs online, a suitable selection must be applied to ensure the full physics event occurs within the current DAQ event; proceeding and preceding events are inaccessible to the DAQ during running.  This is achieved by noting whether or not a trigger occurred (i.e. microslices contain nanoslices) when reformatting the RCE data in DataReformatter, and which microslice it occurred in.  For the event display, a triggered event is only useful if the trigger occurred within a certain range (e.g. microslice 5 to microslice 10); this ensures all the filled microslices are present within the current millislice.  The event displayed is then filled for a given range of microslices around the trigger to capture all the actual physics data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Representing the Data}\label{RepresentingEVDData}

[This section requires background of reconstruction -- this will be the previous chapter.]

Due to the wrapped nature of the induction wires in the 35~ton, and disambiguation being impossible without full reconstruction, it makes little sense to look at charge deposited on these planes.  This results in only the collection planes being useful for showing the data in this way, meaning just one dimension.  A second dimension is possible if the view is changed to show a representation of the TPC from above and using the drift time as a coordinate.  This requires the two centre APAs be shown together as one combined readout structure.  A `global collection wire' is defined by numbering the wires across the APAs, leaving a space for the gaps in between, and used to represent the dimension across the TPC.  The drift time, in ticks, represents the second spacial coordinate once charge collected in the short drift volume has been corrected to a negative tick.

By working with the system used to record pedestal values of the channels, it is possible to perform an approximate pedestal subtraction on the data.  Whenever a pedestal run is performed by a shifter, a text file containing all the calculated pedestal values for each channel is created and subsequently uploaded to a database for offline use.  By making sure a copy of the most recent pedestal file is always available to the monitoring framework, it is possible to always represent the charge as accurately as possible.  It is then ensured the pedestal-subtracted ADC values are within the range $0-250$ to limit the noisy channels and correct for any accidental negative charge.  Finally, given the relatively low signal-to-noise ratio, it was decided a grey-scale image showed the best resolution for seeing tracks traverse the cryostat.

An example event display is shown in Figure \ref{fig:EVD}.

\begin{figure}[p]
  \centering
  \includegraphics[width=14cm]{evd.png}
  \caption[Example online event display made by the Online Monitoring framework]{Example online event display made as part of the online monitoring framework for run 14306 (2nd March, 2016).  The view is from the top of the detector looking down; the red lines represent the spaces between the APAs and the blue line the location of the APA frames, separating the long and short drift regions.}
  \label{fig:EVD}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Monitoring Web Interface}\label{sec:WebInterface}

The output of the monitoring is vital in assuring the experiment continues to take high quality, analysable data.  To facilitate the monitoring, a web interface was developed to enable all useful information to be displayed on the web and allow convenient, universal access.  This interface, along with the complementary web page, was relatively basic but certainly functional and delivered all expected of it for the purposes of a short prototype run.  The method of automating the transfer of the monitoring data from where it was saved by the DAQ process to somewhere accessible by the web server is briefly described in Section \ref{sec:AutomatedDataTransfer} and the web page itself discussion in Section \ref{sec:WebPage}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Automated Data Transfer}\label{sec:AutomatedDataTransfer}

The most complicated part of the web interface was ensuring the monitoring output were available in the correct place when needed.  This is achieved using a combination of disk mounting and automated scripts, demonstrated in Figure \ref{fig:WebInterface}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=12cm]{webInterface.eps}
  \caption[Schematic showing the interface between the online monitoring system and the web]{Schematic showing the interface between the online monitoring system and the web.  The DAQ machines are shown as rectangles with their disks represented as cylinders.  Connections between a node and a disk are shown as straight lines, with dotted lines representing processes running on the machine.}
  \label{fig:WebInterface}
\end{figure}

The DAQ aggregator processes run on the lbnedaq6 and lbnedaq7 nodes, requiring any saved output be put in a place which is accessible to these machines.  The solution which was developed was to mount a data disk belonging to lbne35t-gateway02 onto these private nodes and save all relevant information there.  The constraints placed on the configuration by the DAQ group, which preferred nothing other than DAQ processes to run on lbne35t-gateway01, required a second gateway node to be utilised to move the files off the private network.  Using lbne35t-gateway02 also allowed the Fermilab web area to be mounted, with an automated job utilised to copy the monitoring output from the data disk to the specific area on the web server.  The frequency of this job, 30s, defined the maximum latency one could expect between data being written out and images appearing online.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Web Page}\label{sec:WebPage}

The web page was hosted at FNAL and located at lbne-dqm.fnal.gov.  The method in the monitoring framework used to write out all the output also wrote and saved all the HTML used to allow images of the plots to be displayed on the web.  This HTML is copied, along with all the images and data files, to the web area as discussed above in Section \ref{sec:AutomatedDataTransfer}.  The web page was basic but performed all required for use in the 35~ton; it had dedicated pages for all the data quality monitoring information and the online event display (the nearline monitoring was also hosted at this website but is not described here).  See Figure \ref{fig:WebPage} for a demonstration of web page and example navigation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=14cm]{webPage.png}
  \caption[Web page for online monitoring and event displays]{Demonstration of the web page developed to display information produced by the online monitoring and event display.  The pages are written in HTML and allowed prompt and convenient feedback directly from the DAQ be accessed anywhere and assist in remote monitoring of the experiment.  All previous runs are also kept on the website for reference.}
  \label{fig:WebPage}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Online Monitoring Summary}

The monitoring, with web support, was imperative for the success of the 35~ton.  During the ongoing vertical slice tests during summer 2015, the majority of the setup was in place and enabled progress in testing and signing off the APAs to be completed months faster than it otherwise would have been.  During this time, and also during commissioning, this framework was the only way of analysing the data without reading it into LArSoft and writing specific software.  Overall, the framework provided essential feedback and contributed positively towards DAQ uptime during the data taking period.  It is currently in the process of being adapted for future use in DUNE, specifically as part of the ProtoDUNE DAQ for the run in 2018.
